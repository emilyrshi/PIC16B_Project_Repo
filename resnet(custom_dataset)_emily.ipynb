{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75687aae",
   "metadata": {},
   "source": [
    "From : https://www.youtube.com/watch?v=mn5QDKQ54dQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad6dc0b-c30f-4300-9e62-dad86e69148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982f34b8-1c35-4767-8c19-5ee446aab58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # randomly resize and crop all the input images to 224 pixel size\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        # transform the images by horizontally flipping them\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # converting all the image data to a tensor BECAUSE PyTorch accepts the data in the form of tensor\n",
    "        # this is a MANDATORY step\n",
    "        transforms.ToTensor(),\n",
    "        # normalizing the data so that our whole input data, training data can be on the same or similar scales\n",
    "        # each array contains the RGB values (each value is a channel)\n",
    "        # so we are working on colored images \n",
    "        # second array is performing standard deviation on it\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # repeat steps from training data to testing (validation) data as well\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# performing this augmentation allows us to provide a variety of different scenarios to a model so that our model will \n",
    "# learn to generalize in a better way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c86d400-80fe-41bf-bd98-c4d92f6036b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "# image_classification notebook is in the same source/repos directory in local and Jupyter Notebook directory\n",
    "# this Notebook has to be in the same location as the dataset folder\n",
    "data_dir = 'dataset'\n",
    "\n",
    "# Create data loaders\n",
    "# data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "#image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678a888b-f751-4e87-be61-55a92be361c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 420, 'test': 420}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d', 'e', 'h', 'l', 'o', 'r', 'w']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are loading the data into mini batches (batch size of 4)\n",
    "# shuffle means while training the data it will shuffle the data\n",
    "# num_workers means parallelizing the process (4 different processes can work at the same time)\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "# number of images per \"train\" and \"test\" folder\n",
    "print(dataset_sizes)\n",
    "\n",
    "# number of folders and their names in the \"train\" and \"test\" folders\n",
    "# these are the letters we are working with\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f4d89f-c66d-473a-b526-d4cbb991339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 model\n",
    "# pretrained = true means we are using a pretrained model\n",
    "# picking the model from torch vision model zoo\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer and then fine tune this model on our custom data set \n",
    "# to detect whether the given image to a model is what letter\n",
    "# freezing all layers except the final classification layers which is responsible for performing classfication\n",
    "for name, param in model.named_parameters():\n",
    "    # if the paramater contains this FC (FC means fully connected layer), then set the required grads equal to \n",
    "    # true\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    # if FC is not in the parameter, then set the required grads equal to false\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    # what happens is wherever fc parameter is present, all those layers will be trained because we are setting\n",
    "    # the value true over there and wherever we have written false, all those layers will be freeze. this is how\n",
    "    # we freeze all the layers and we can only fine tune the final layer\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# whenever working on image classification, this is most commonly method to calculate loss. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer is a stochastic gradient descent optimizer (lr is learning range and momentum)\n",
    "# CAN CHANGE THESE VALUES to test how the model performs\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# sending all the models here to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe788bcb-77d7-4c86-9f4b-80868208b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.3654 Acc: 0.1524\n",
      "test Loss: 1.8379 Acc: 0.3476\n",
      "train Loss: 1.8918 Acc: 0.3405\n",
      "test Loss: 1.0831 Acc: 0.5833\n",
      "train Loss: 1.6896 Acc: 0.4167\n",
      "test Loss: 0.8927 Acc: 0.7238\n",
      "train Loss: 1.6497 Acc: 0.4429\n",
      "test Loss: 0.6354 Acc: 0.8238\n",
      "train Loss: 1.4886 Acc: 0.4595\n",
      "test Loss: 0.5863 Acc: 0.8048\n",
      "train Loss: 1.3913 Acc: 0.5167\n",
      "test Loss: 0.5167 Acc: 0.8405\n",
      "train Loss: 1.2768 Acc: 0.5476\n",
      "test Loss: 0.4021 Acc: 0.8762\n",
      "train Loss: 1.2814 Acc: 0.5357\n",
      "test Loss: 0.4010 Acc: 0.8762\n",
      "train Loss: 1.3050 Acc: 0.5238\n",
      "test Loss: 0.4085 Acc: 0.8571\n",
      "train Loss: 1.3541 Acc: 0.5119\n",
      "test Loss: 0.4406 Acc: 0.8381\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        # these two variables are used to store the loss and the correct \n",
    "        # predictions inside every epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            # inputs means the image \n",
    "            inputs = inputs.to(device)\n",
    "            # labels means the output label (the class name)\n",
    "            labels = labels.to(device)\n",
    "            # all being sent to device because the model is on the device.\n",
    "            # which device depends on whether this is running on CPU or GPU\n",
    "            \n",
    "            # clear the gradients from the previous iterations \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # if phase is train, then we are using the model to make predictions \n",
    "                # and providing the inputs which are the images \n",
    "                outputs = model(inputs)\n",
    "                # prediction of the model is stored in outputs\n",
    "                # this line is responsible for showing you the predictions \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # now providing the outputs which are the predictions \n",
    "                # \"outputs\" is the predictions of the model and \"labels\" is the actual labels\n",
    "                # so we are comparing the output label and our current label, on the basis\n",
    "                # of that we are getting the loss value \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # how deep learning/CNN works: \n",
    "                # there is backward pass and forward pass. first we go the forward pass. \n",
    "                # forward pass means you provide input to the model, the model processes\n",
    "                # the image and provide you the corresponding label. then in back propogation, \n",
    "                # we have backward pass. during backward pass, we calculate the gradients and \n",
    "                # then update the weights and after that we again use the forward pass to work\n",
    "                # on the model again, train it with the updated weights. \n",
    "                \n",
    "                # if the phase is train, we will perform backward pass. \n",
    "                if phase == 'train':\n",
    "                    # in backward pass, we are calculating the gradients\n",
    "                    loss.backward()\n",
    "                    # and then we are updating the weights on the basis of calculated gradients \n",
    "                    optimizer.step()\n",
    "\n",
    "            # storing all the losses and all the correct predictions \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        # these two lines helping you to see the epoch loss and epoch accuracy\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c696019-b8ca-4cac-9f5f-d350e7285265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'letter_classification_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057bb6d-9592-4809-af3a-fb05ef595477",
   "metadata": {},
   "source": [
    "# Classification on Unseen Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f514-6ad8-46cb-b9b6-f56f7da59476",
   "metadata": {},
   "source": [
    "To use the saved model to classify unseen images, you need to load the model and then apply it to the new images for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a509530-539e-4e35-8a1b-181215da720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# the pretrained model is trained on imageNet dataset with a thousand classes \n",
    "# freezing all the layers except for the last layer means we are using transfer learning which\n",
    "# means the model is already trained on some data \n",
    "# that means this model is ready to extract the features so we can use that knowledge from the \n",
    "# pretrained model to extract the feature and in the final layer we are only using the two \n",
    "# neurons which are responsible for telling us what letter class it is\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('letter_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "# 7 for the number of classes\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 7)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557a6cc-c536-44b7-bfad-52f3d33e1480",
   "metadata": {},
   "source": [
    "Prepare your new image for classification. You should use the same data transformations you used during training. Here's an example of how to prepare an image for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1445dcf2-2311-4025-80e9-2b57ccf559e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT steps from training data also on this testing data \n",
    "\n",
    "# Load and preprocess the unseen image\n",
    "image_path = 'E_test_image.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "input_tensor = preprocess(image)\n",
    "# adding a batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15505026-2f4d-4072-9ba1-c3dc265a43b1",
   "metadata": {},
   "source": [
    "Perform inference using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aefe032-46f2-4f08-984e-39584a83cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: e\n"
     ]
    }
   ],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657ea260-40c3-4a69-bb48-8de4fe305717",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display the image with the predicted class name\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Display the image with the predicted class name\n",
    "image = np.array(image)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.text(10, 10, f'Predicted: {predicted_class_name}', fontsize=12, color='white', backgroundcolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdda75-618a-4b16-884a-6b30d9a7d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
