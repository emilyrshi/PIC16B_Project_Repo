# -*- coding: utf-8 -*-
"""CNN_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QEuBhEA-2F5ILWFCHxKIq5G7ZOy9KRj_

# Set Up
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

! pip install jovian --upgrade -q

import torch
from torch.utils.data import TensorDataset, DataLoader, random_split

from PIL import Image
import pandas as pd

from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

import torch.nn as nn
import torch.nn.functional as F

from torchvision.utils import make_grid

import jovian

from sklearn.model_selection import train_test_split

from google.colab import drive

drive.mount('/content/drive')
os.chdir('/content/drive/My Drive/PIC16B_PROJ/CollectedImages')

sign_language_url = "https://raw.githubusercontent.com/emilyrshi/PIC16B_Project_Repo/6e27757ed9a716812ef7acfc855be8521285a464/SignLanguage_CSV/sign_language_dataset.csv"

data = pd.read_csv(sign_language_url, index_col = False)

data = pd.read_csv(sign_language_url)
data.drop('Unnamed: 0', axis=1, inplace=True)

# Reorder columns with the specified column at the beginning
reordered_columns = ['Letter'] + [col for col in data.columns if col != 'Letter']
data = data[reordered_columns]
data.rename(columns={'Letter': 'label'}, inplace=True)
# Define a dictionary to map each letter to its numerical value
# alphabet_mapping = {'D': 4, 'E': 5, 'H': 8, 'L': 12, 'O': 15, 'R': 18, 'W': 23}
alphabet_mapping = {'D': 0, 'E': 1, 'H': 2, 'L': 3, 'O': 4, 'R': 5, 'W': 6}
# Map the values in the "label" column using the dictionary
data['label'] = data['label'].map(alphabet_mapping)

# Split into train and test sets (80% train, 20% test)
train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)

train_labels = torch.tensor(train_df['label'].values, dtype=torch.long)
test_labels = torch.tensor(test_df['label'].values, dtype=torch.long)

# Print the data types of the new tensors
print("Train labels data type:", train_labels.dtype)
print("Test labels data type:", test_labels.dtype)
# Display the shapes of the datasets
print("Train set shape:", train_df.shape)
print("Test set shape:", test_df.shape)
unique_values = train_df['label'].unique()

# Print unique values
for value in unique_values:
    print(value)

train_df

Classes = list('HELOWRD')
Classes

def dataframe_to_nparray(train_df, test_df):
    train_df1 = train_df.copy(deep = True)
    test_df1 = test_df.copy(deep = True)
    train_images = train_df1.iloc[:, 1:].to_numpy(dtype = 'float32')
    test_images = test_df1.iloc[:, 1:].to_numpy(dtype = 'float32')
    return train_images,test_images

train_img, test_img = dataframe_to_nparray(train_df, test_df)
train_labels = train_df['label'].values
test_labels = test_df['label'].values

train_img.size

train_images_shaped = train_img.reshape(train_img.shape[0],1,28,28)
test_images_shaped = test_img.reshape(test_img.shape[0],1,28,28)

train_images_tensors = torch.from_numpy(train_images_shaped)
train_labels_tensors = torch.from_numpy(train_labels)

test_images_tensors = torch.from_numpy(test_images_shaped)
test_labels_tensors = torch.from_numpy(test_labels)

# pytorch dataset
train_ds_full = TensorDataset(train_images_tensors, train_labels_tensors) #this dataset will further devided into validation dataset and training dataset
test_ds = TensorDataset(test_images_tensors, test_labels_tensors)

img, label = train_ds_full[0]
print(img.shape, label)
img

# Hyperparmeters
batch_size = 25
learning_rate = 0.001

# Other constants
in_channels = 1
input_size = in_channels * 28 * 28
num_classes = 7

"""Training and Validation Dataset"""

random_seed = 11
torch.manual_seed(random_seed);

val_size = 200
train_size = len(train_ds_full) - val_size

train_ds, val_ds = random_split(train_ds_full, [train_size, val_size,])
len(train_ds), len(val_ds), len(test_ds)

def show_image(image, label):
    print("Alphabet: ", Classes[label.item()])
    plt.imshow(image.view(28,28))

# show_image(*train_ds[8])

# show_image(*train_ds[6363])

"""Loading the training, validation, and test dataset into batches."""

train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)
test_dl = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)

for img, label in train_dl:
    print(img.size())
    break

"""# CNN"""

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class ASLBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss

    def validation_step(self, batch):
        images, labels = batch
        out = self(images)                    # Generate predictions
        loss = F.cross_entropy(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['train_loss'], result['val_loss'], result['val_acc']))

class ASLCNNModel(ASLBase):
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.network = nn.Sequential(
            nn.Conv2d(in_channels, 28, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(28, 28, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),     #image size : 28*14*14

            nn.Conv2d(28, 56, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(56, 56, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # image size : 56*7*7

            nn.Flatten(),
            nn.Linear(56*7*7, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes))

    def forward(self, xb):
        return self.network(xb)

in_channels, num_classes

model = ASLCNNModel(in_channels, num_classes)
model

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device

    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl:
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device

train_dl = DeviceDataLoader(train_dl, device)
val_dl = DeviceDataLoader(val_dl, device)
test_dl = DeviceDataLoader(test_dl, device)
to_device(model, device);

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        # Training Phase
        model.train()
        train_losses = []
        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        # Validation phase
        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_losses).mean().item()
        model.epoch_end(epoch, result)
        history.append(result)
    return history

model = to_device(ASLCNNModel(in_channels, num_classes), device)

evaluate(model, val_dl)

"""So, initially this CNN model gives us 4% accuracy and a large validation loss of 3.26."""

num_epochs = 10
opt_func = torch.optim.Adam

history = fit(num_epochs, 0.001 , model, train_dl, val_dl, opt_func)

def plot_accuracies(history):
    accuracies = [x['val_acc'] for x in history]
    plt.plot(accuracies, '-x')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Accuracy vs. No. of epochs');

plot_accuracies(history)

def plot_losses(history):
    train_losses = [x.get('train_loss') for x in history]
    val_losses = [x['val_loss'] for x in history]
    plt.plot(train_losses, '-bx')
    plt.plot(val_losses, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    plt.title('Loss vs. No. of epochs');

plot_losses(history)

"""# Testing with Test Images"""

# Testing with test dataloader
result = evaluate(model, test_dl)
result

def predict_image(img, model):
    # Convert to a batch of 1
    xb = to_device(img.unsqueeze(0), device)
    # Get predictions from model
    yb = model(xb)
    # Pick index with highest probability
    _, preds  = torch.max(yb, dim=1)
    # Retrieve the class label
    return preds[0].item()

img, label = test_ds[0]
plt.imshow(img.view(28,28), cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test_ds[6762]
plt.imshow(img.view(28,28), cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test_ds[3535]
plt.imshow(img.view(28,28), cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test_ds[23]
plt.imshow(img.view(28,28), cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

"""# Saving the Model"""

torch.save(model.state_dict(), '3-asl-cnn.pth')

jovian.commit(project= project_name)