{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6280ff",
   "metadata": {},
   "source": [
    "OVERALL From : https://www.youtube.com/watch?v=mn5QDKQ54dQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad6dc0b-c30f-4300-9e62-dad86e69148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98ab67",
   "metadata": {},
   "source": [
    "following about data augmentation: https://pytorch.org/vision/main/transforms.html\n",
    "\n",
    "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982f34b8-1c35-4767-8c19-5ee446aab58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define data transformations for data augmentation and normalization\n",
    "# # https://pytorch.org/vision/0.9/transforms.html\n",
    "\n",
    "# # Link: https://neptune.ai/blog/data-augmentation-in-python\n",
    "# # Data augmentation is a technique that can be used to artificially\n",
    "# # expand the size of a training set by creating modified data from the\n",
    "# # existing one. It is a good practice to use DA if you want to prevent \n",
    "# # overfitting, or the initial dataset is too small to train on, or even \n",
    "# # if you want to squeeze better performance from your model.\n",
    "\n",
    "# # Benefits of data augmenting:\n",
    "# # 1. prevent overfitting\n",
    "# # 2. improving the performance of the model \n",
    "\n",
    "# # Techniques of data augmentation used here (for images):\n",
    "# # 1. geometric transformations: randomly flip, crop, rotate images \n",
    "# # 2. color space transformations: change RGB color channels\n",
    "\n",
    "# transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         # ADDED IN FROM https://pytorch.org/vision/0.9/transforms.html\n",
    "    \n",
    "#         # horizontally flip the image randomly with given probability\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         # randomly resize and crop all the input images to 224 pixel size\n",
    "#         transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#         # rotate the image by angle\n",
    "#         transforms.RandomRotation(degrees=90),\n",
    "#         # randomly convert image to grayscale with probability of p\n",
    "#         # (default 0.1)\n",
    "#         transforms.RandomGrayscale(p=0.1),\n",
    "#         # performs a random persepctive transformation of the image with\n",
    "#         # given probability \n",
    "#         transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "#         # vertically flip the image randomly with given probability\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         # converting all the image data to a tensor BECAUSE PyTorch accepts\n",
    "#         # the data in the form of tensor\n",
    "#         # when image is transformed into PyTorch tensor, the pixel values \n",
    "#         # are scaled between 0.0 and 1.0\n",
    "#         # converts the PIL image with a pixel range of [0, 255] to a PyTorch\n",
    "#         # FloatTensor of shape (C, H, W) with range [0.0, 1.0]\n",
    "#         # this is a MANDATORY step\n",
    "#         transforms.ToTensor(),\n",
    "#         # Link : https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "#         # normalizing images means transforming them into such values that\n",
    "#         # the means and std dev of the image become 0.0 and 1.0 respectively\n",
    "#         # normalization helps get data within a range and reduces the \n",
    "#         # skewness which helps learn faster and better, and can also tackle\n",
    "#         # the diminishing and exploding gradients problem\n",
    "#         # first parameter is the mean, second is the std\n",
    "#         # the mean and std of ImageNet are: mean = [0.485, 0.465, 0.406]\n",
    "#         # and std = [0.229, 0.224, 0.225]\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     # repeat steps from training data to testing (validation) data \n",
    "#     'test': transforms.Compose([    \n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#         transforms.RandomRotation(degrees=90),\n",
    "#         transforms.RandomGrayscale(p=0.1),\n",
    "#         transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "############################################3333\n",
    "# print(transforms)\n",
    "# # performing this augmentation allows us to provide a variety of different scenarios to a model so that our model will \n",
    "# # learn to generalize in a better way\n",
    "\n",
    "# Define data transformations for data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # randomly resize and crop all the input images to 224 pixel size\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        # transform the images by horizontally flipping them\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # converting all the image data to a tensor BECAUSE PyTorch accepts the data in the form of tensor\n",
    "        # this is a MANDATORY step\n",
    "        transforms.ToTensor(),\n",
    "        # normalizing the data so that our whole input data, training data can be on the same or similar scales\n",
    "        # each array contains the RGB values (each value is a channel)\n",
    "        # so we are working on colored images \n",
    "        # second array is performing standard deviation on it\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # repeat steps from training data to testing (validation) data as well\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c86d400-80fe-41bf-bd98-c4d92f6036b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 700\n",
      "    Root location: dataset\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory\n",
    "# image_classification notebook is in the same source/repos directory in local and Jupyter Notebook directory\n",
    "# this Notebook has to be in the same location as the dataset folder\n",
    "dataset_directory = 'dataset'\n",
    "\n",
    "# # Create data loaders\n",
    "# # data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# # inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "# datasets_images = {img: datasets.ImageFolder(os.path.join(dataset_directory, img), transforms[img]) \n",
    "#                    for img in dataset_directory}\n",
    "\n",
    "# print(datasets_images)\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "# data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "dataset_imgs = datasets.ImageFolder(\n",
    "                              root = dataset_directory,\n",
    "                              transform = data_transforms[\"train\"]\n",
    "                       )\n",
    "print(dataset_imgs)\n",
    "\n",
    "# # # 80% of the d's go into train\n",
    "# train_size = int(0.8 * len(datasets_images['train']))\n",
    "# test_size = len(datasets_images['train']) - train_size\n",
    "# # datasets_images['train'] contains all original images in their respective folders\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(datasets_images['train'], [train_size, test_size])\n",
    "# print(len(train_dataset))\n",
    "# print(len(test_dataset))\n",
    "############################################\n",
    "# data_dir = 'dataset'\n",
    "\n",
    "# # Create data loaders\n",
    "# # data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# # inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "# #image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27a275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 0, 'e': 1, 'h': 2, 'l': 3, 'o': 4, 'r': 5, 'w': 6}\n",
      "{0: 'd', 1: 'e', 2: 'h', 3: 'l', 4: 'o', 5: 'r', 6: 'w'}\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "# class to index mapping\n",
    "print(dataset_imgs.class_to_idx)\n",
    "\n",
    "# {'airplane': 0, 'car': 1, 'cat': 2, 'dog': 3, 'flower': 4, 'fruit': 5, 'motorbike': 6, 'person': 7}\n",
    "\n",
    "# index to class mapping : reverse of class to index mapping\n",
    "idx_to_class = {v: k for k, v in dataset_imgs.class_to_idx.items()}\n",
    "print(idx_to_class) # {0: 'airplane', 1: 'car', 2: 'cat', 3: 'dog', 4: 'flower', 5: 'fruit', 6: 'motorbike', 7: 'person'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c1cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes:  {'d': 100, 'e': 100, 'h': 100, 'l': 100, 'o': 100, 'r': 100, 'w': 100}\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    count_dict = {k:0 for k,v in dataset_imgs.class_to_idx.items()} # initialise dictionary\n",
    "    \n",
    "    for input, label in dataset:\n",
    "        label = idx_to_class[label]\n",
    "        count_dict[label] += 1\n",
    "            \n",
    "    return count_dict\n",
    "\n",
    "print(\"Distribution of classes: \", get_class_distribution(dataset_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1477e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the  class distribution\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(dataset_imgs)]).melt(), \n",
    "#             x = \"variable\", y=\"value\", hue=\"variable\").set_title('Class Distribution of the natural_img_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa89bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "420\n",
      "140\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "train_percent = int(len(dataset_imgs)*0.6)\n",
    "val_percent = int(len(dataset_imgs)*0.2)\n",
    "test_percent = int(len(dataset_imgs)*0.2)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_imgs, (train_percent, val_percent, test_percent))\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset_imgs, (420, 140, 140))\n",
    "print(len(dataset_imgs)) # length of the dataset\n",
    "print(len(train_dataset)) # length of the train division\n",
    "print(len(val_dataset)) # length of the validation division\n",
    "print(len(test_dataset)) # length of the test division\n",
    "\n",
    "# 700\n",
    "# 420\n",
    "# 140\n",
    "# 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc98144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train_loader: 105\n",
      "Length of the val_loader: 35\n",
      "Length of the test_loader: 35\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "# # first parameter: training_data\n",
    "# # shuffle means while training the data it will shuffle the data\n",
    "# # num_workers means parallelizing the process (4 different processes can work at the same time)\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=4, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=False, batch_size=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=4)\n",
    "\n",
    "# length of dataloader = overall dataset len / batch size\n",
    "print(\"Length of the train_loader:\", len(train_loader))\n",
    "print(\"Length of the val_loader:\", len(val_loader))\n",
    "print(\"Length of the test_loader:\", len(test_loader))\n",
    "\n",
    "# Length of the train_loader: 78\n",
    "# Length of the val_loader: 16\n",
    "# Length of the test_loader: 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c98aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,10))\n",
    "# sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(train_dataset)]).melt(),\n",
    "#             x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train Set')\n",
    "# sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(val_dataset)]).melt(), \n",
    "#             x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('Val Set')\n",
    "# sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(test_dataset)]).melt(),\n",
    "#             x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[2]).set_title('Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f4d89f-c66d-473a-b526-d4cbb991339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 model\n",
    "# pretrained = true means we are using a pretrained model\n",
    "# picking the model from torch vision model zoo\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer and then fine tune\n",
    "# this model on our custom data set to detect whether the given image to a\n",
    "# model is what letter\n",
    "# freezing all layers except the final classification layers which is \n",
    "# responsible for performing classfication\n",
    "# this for loop line specifically: return an iterator over module parameters,\n",
    "# yielding both the name of the parameter as well as the parameter itself\n",
    "for name, param in model.named_parameters():\n",
    "    # if the paramater contains this FC (FC means fully connected layer), then set the required grads equal to \n",
    "    # true\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    # if FC is not in the parameter, then set the required grads equal to false\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    # what happens is wherever fc parameter is present, all those layers will be trained because we are setting\n",
    "    # the value true over there and wherever we have written false, all those layers will be freeze. this is how\n",
    "    # we freeze all the layers and we can only fine tune the final layer\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# whenever working on image classification, this is most commonly method to calculate loss. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer is a stochastic gradient descent optimizer (lr is learning range and momentum)\n",
    "# CAN CHANGE THESE VALUES to test how the model performs\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# sending all the models here to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe788bcb-77d7-4c86-9f4b-80868208b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.2547 Acc: 0.1595\n",
      "test Loss: 1.9588 Acc: 0.3119\n",
      "train Loss: 1.9457 Acc: 0.2690\n",
      "test Loss: 1.7701 Acc: 0.4095\n",
      "train Loss: 1.7704 Acc: 0.3571\n",
      "test Loss: 1.4010 Acc: 0.5167\n",
      "train Loss: 1.7139 Acc: 0.4262\n",
      "test Loss: 1.1969 Acc: 0.5690\n",
      "train Loss: 1.5119 Acc: 0.4619\n",
      "test Loss: 1.1215 Acc: 0.6048\n",
      "train Loss: 1.3882 Acc: 0.5143\n",
      "test Loss: 1.0192 Acc: 0.6714\n",
      "train Loss: 1.5253 Acc: 0.4810\n",
      "test Loss: 1.1516 Acc: 0.6214\n",
      "train Loss: 1.3139 Acc: 0.5286\n",
      "test Loss: 1.0364 Acc: 0.6286\n",
      "train Loss: 1.3666 Acc: 0.5024\n",
      "test Loss: 1.0496 Acc: 0.6619\n",
      "train Loss: 1.3404 Acc: 0.5310\n",
      "test Loss: 0.9158 Acc: 0.6810\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        # these two variables are used to store the loss and the correct \n",
    "        # predictions inside every epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs means the image \n",
    "            inputs = inputs.to(device)\n",
    "            # labels means the output label (the class name)\n",
    "            labels = labels.to(device)\n",
    "            # all being sent to device because the model is on the device.\n",
    "            # which device depends on whether this is running on CPU or GPU\n",
    "            \n",
    "            # clear the gradients from the previous iterations \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # if phase is train, then we are using the model to make predictions \n",
    "                # and providing the inputs which are the images \n",
    "                outputs = model(inputs)\n",
    "                # prediction of the model is stored in outputs\n",
    "                # this line is responsible for showing you the predictions \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # now providing the outputs which are the predictions \n",
    "                # \"outputs\" is the predictions of the model and \"labels\" is the actual labels\n",
    "                # so we are comparing the output label and our current label, on the basis\n",
    "                # of that we are getting the loss value \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # how deep learning/CNN works: \n",
    "                # there is backward pass and forward pass. first we go the forward pass. \n",
    "                # forward pass means you provide input to the model, the model processes\n",
    "                # the image and provide you the corresponding label. then in back propogation, \n",
    "                # we have backward pass. during backward pass, we calculate the gradients and \n",
    "                # then update the weights and after that we again use the forward pass to work\n",
    "                # on the model again, train it with the updated weights. \n",
    "                \n",
    "                # if the phase is train, we will perform backward pass. \n",
    "                if phase == 'train':\n",
    "                    # in backward pass, we are calculating the gradients\n",
    "                    loss.backward()\n",
    "                    # and then we are updating the weights on the basis of calculated gradients \n",
    "                    optimizer.step()\n",
    "\n",
    "            # storing all the losses and all the correct predictions \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        # these two lines helping you to see the epoch loss and epoch accuracy\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c696019-b8ca-4cac-9f5f-d350e7285265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'letter_classification_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057bb6d-9592-4809-af3a-fb05ef595477",
   "metadata": {},
   "source": [
    "# Classification on Unseen Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f514-6ad8-46cb-b9b6-f56f7da59476",
   "metadata": {},
   "source": [
    "To use the saved model to classify unseen images, you need to load the model and then apply it to the new images for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a509530-539e-4e35-8a1b-181215da720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# the pretrained model is trained on imageNet dataset with a thousand classes \n",
    "# freezing all the layers except for the last layer means we are using transfer learning which\n",
    "# means the model is already trained on some data \n",
    "# that means this model is ready to extract the features so we can use that knowledge from the \n",
    "# pretrained model to extract the feature and in the final layer we are only using the two \n",
    "# neurons which are responsible for telling us what letter class it is\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('letter_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "# 7 for the number of classes\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 7)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557a6cc-c536-44b7-bfad-52f3d33e1480",
   "metadata": {},
   "source": [
    "Prepare your new image for classification. You should use the same data transformations you used during training. Here's an example of how to prepare an image for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445dcf2-2311-4025-80e9-2b57ccf559e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT steps from training data also on this testing data \n",
    "\n",
    "# Load and preprocess the unseen image\n",
    "image_path = 'E_test_image.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "input_tensor = preprocess(image)\n",
    "# adding a batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15505026-2f4d-4072-9ba1-c3dc265a43b1",
   "metadata": {},
   "source": [
    "Perform inference using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefe032-46f2-4f08-984e-39584a83cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ea260-40c3-4a69-bb48-8de4fe305717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Display the image with the predicted class name\n",
    "image = np.array(image)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.text(10, 10, f'Predicted: {predicted_class_name}', fontsize=12, color='white', backgroundcolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdda75-618a-4b16-884a-6b30d9a7d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
