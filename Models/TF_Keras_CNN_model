# -*- coding: utf-8 -*-
"""CNN_Model_working.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nGC2xET1V9dC1SEg_FPxYbuCN9prm6F6
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from keras.models import load_model
from keras.preprocessing import image
import matplotlib.pyplot as plt
from google.colab import drive
import os
import cv2

drive.mount('/content/drive')
os.chdir('/content/drive/My Drive/PIC16B_PROJ/PredictionImages')

sign_language_url = "https://raw.githubusercontent.com/emilyrshi/PIC16B_Project_Repo/6e27757ed9a716812ef7acfc855be8521285a464/SignLanguage_CSV/sign_language_dataset.csv"

sign_language_data = pd.read_csv(sign_language_url, index_col = False)
sign_language_data

# Separate image features and target labels
X = sign_language_data.iloc[:, 1:785]
y = sign_language_data['Letter']

# Converting data to numpy arrays
X = X.to_numpy()
y = y.to_numpy()

# Reshape input features into 4 dimensional tensor
X = X.reshape(-1, 28, 28, 1)

# Convert target labels to one-hot encoding
y = pd.get_dummies(y)
y = y.to_numpy()

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = 42) # random_State = 42

cnn_one_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(7, activation='softmax')  # number of classes (7 letters)
])

# Compiling the model
cnn_one_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

cnn_one_history = cnn_one_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))

plt.plot(cnn_one_history.history["accuracy"], label = "training")
plt.plot(cnn_one_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()

cnn_two_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.Dense(7, activation='softmax')
])

# Compiling the model
cnn_two_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

cnn_two_history = cnn_two_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))

plt.plot(cnn_two_history.history["accuracy"], label = "training")
plt.plot(cnn_two_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()

cnn_three_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.Dense(7, activation='softmax')
])

# Compiling the model
cnn_three_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

cnn_three_history = cnn_three_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))

plt.plot(cnn_three_history.history["accuracy"], label = "training")
plt.plot(cnn_three_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()

"""# **Evaluating Model on Testing Data**"""

# Evaluate accuracy on testing
loss, accuracy = cnn_three_model.evaluate(X_val, y_val)

"""## **Classification on Unseen Image**"""

# Define a dictionary of the labels
labels = {
    0: 'D',
    1: 'E',
    2: 'H',
    3: 'L',
    4: 'O',
    5: 'R',
    6: 'W'
}

# Loading images and cleaning image
def load_image(img_path, show=False):
    # Resizing to 28 x 28 and changing to grayscale
    img = image.load_img(img_path, target_size=(28, 28), color_mode='grayscale')
    # Converting to array
    img_tensor = image.img_to_array(img)
    img_tensor = np.expand_dims(img_tensor, axis=0)
    img_tensor /= 255.

    if show:
        plt.imshow(img_tensor[0], cmap='gray')
        plt.axis('off')
        plt.show()

    return img_tensor

if __name__ == "__main__":
    # Load model for prediction
    model = cnn_one_model
    # Location of image
    folder_dir = '/content/drive/My Drive/PIC16B_PROJ/PredictionImages/'
      # Where our folder is located
      # Please change folder_dir to the location of the images you would like to test

    # Loading images from the directory
    for filename in os.listdir(folder_dir):
        if filename.endswith(".jpg"):
            img_path = os.path.join(folder_dir, filename)
            new_image = load_image(img_path)
            # Making a prediction
            pred_prob = model.predict(new_image)
            # Outputing the probability
            print(pred_prob)
            # Finding the highest probability
            pred_class = np.argmax(pred_prob, axis=1)
            # Getting the letter corresponding to the highest probability
            predicted_letter = labels[pred_class[0]]
            print("Prediction for", filename, ":", predicted_letter)
