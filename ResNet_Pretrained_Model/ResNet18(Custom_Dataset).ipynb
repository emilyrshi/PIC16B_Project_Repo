{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d99f9e8",
   "metadata": {},
   "source": [
    "# Image Classification Using Pretrained Model (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe18ca4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad6dc0b-c30f-4300-9e62-dad86e69148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fe99f",
   "metadata": {},
   "source": [
    "## Data Transforms and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982f34b8-1c35-4767-8c19-5ee446aab58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "), 'test': Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# # Define data transformations for data augmentation and normalization\n",
    "# # https://pytorch.org/vision/0.9/transforms.html\n",
    "\n",
    "# # Link: https://neptune.ai/blog/data-augmentation-in-python\n",
    "# # Data augmentation is a technique that can be used to artificially\n",
    "# # expand the size of a training set by creating modified data from the\n",
    "# # existing one. It is a good practice to use DA if you want to prevent \n",
    "# # overfitting, or the initial dataset is too small to train on, or even \n",
    "# # if you want to squeeze better performance from your model.\n",
    "\n",
    "# # Benefits of data augmenting:\n",
    "# # 1. prevent overfitting\n",
    "# # 2. improving the performance of the model \n",
    "\n",
    "# # Techniques of data augmentation used here (for images):\n",
    "# # 1. geometric transformations: randomly flip, crop, rotate images \n",
    "# # 2. color space transformations: change RGB color channels\n",
    "\n",
    "# transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         # ADDED IN FROM https://pytorch.org/vision/0.9/transforms.html\n",
    "    \n",
    "#         # horizontally flip the image randomly with given probability\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         # randomly resize and crop all the input images to 224 pixel size\n",
    "#         transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#         # rotate the image by angle\n",
    "#         transforms.RandomRotation(degrees=90),\n",
    "#         # randomly convert image to grayscale with probability of p\n",
    "#         # (default 0.1)\n",
    "#         transforms.RandomGrayscale(p=0.1),\n",
    "#         # performs a random persepctive transformation of the image with\n",
    "#         # given probability \n",
    "#         transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "#         # vertically flip the image randomly with given probability\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         # converting all the image data to a tensor BECAUSE PyTorch accepts\n",
    "#         # the data in the form of tensor\n",
    "#         # when image is transformed into PyTorch tensor, the pixel values \n",
    "#         # are scaled between 0.0 and 1.0\n",
    "#         # converts the PIL image with a pixel range of [0, 255] to a PyTorch\n",
    "#         # FloatTensor of shape (C, H, W) with range [0.0, 1.0]\n",
    "#         # this is a MANDATORY step\n",
    "#         transforms.ToTensor(),\n",
    "#         # Link : https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "#         # normalizing images means transforming them into such values that\n",
    "#         # the means and std dev of the image become 0.0 and 1.0 respectively\n",
    "#         # normalization helps get data within a range and reduces the \n",
    "#         # skewness which helps learn faster and better, and can also tackle\n",
    "#         # the diminishing and exploding gradients problem\n",
    "#         # first parameter is the mean, second is the std\n",
    "#         # the mean and std of ImageNet are: mean = [0.485, 0.465, 0.406]\n",
    "#         # and std = [0.229, 0.224, 0.225]\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     # repeat steps from training data to testing (validation) data \n",
    "#     'test': transforms.Compose([    \n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "#         transforms.RandomRotation(degrees=90),\n",
    "#         transforms.RandomGrayscale(p=0.1),\n",
    "#         transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "\n",
    "# print(transforms)\n",
    "\n",
    "########################################\n",
    "\n",
    "# Define data transformations for data augmentation and normalization\n",
    "transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # randomly resize and crop all the input images to 224 pixel size\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        # transform the images by horizontally flipping them\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # converting all the image data to a tensor BECAUSE PyTorch accepts the data in the form of tensor\n",
    "        # this is a MANDATORY step\n",
    "        transforms.ToTensor(),\n",
    "        # normalizing the data so that our whole input data, training data can be on the same or similar scales\n",
    "        # each array contains the RGB values (each value is a channel)\n",
    "        # so we are working on colored images \n",
    "        # second array is performing standard deviation on it\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # repeat steps from training data to testing (validation) data as well\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e89ff",
   "metadata": {},
   "source": [
    "## Define Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c86d400-80fe-41bf-bd98-c4d92f6036b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 700\n",
      "    Root location: dataset\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory\n",
    "# image_classification notebook is in the same source/repos directory in local and Jupyter Notebook directory\n",
    "# this Notebook has to be in the same location as the dataset folder\n",
    "dataset_directory = 'dataset'\n",
    "\n",
    "# Create data loaders\n",
    "# data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "dataset_imgs = datasets.ImageFolder(\n",
    "                              root = 'dataset',\n",
    "                              transform = transforms[\"train\"]\n",
    "                       )\n",
    "print(dataset_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8b3050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 0, 'e': 1, 'h': 2, 'l': 3, 'o': 4, 'r': 5, 'w': 6}\n",
      "{0: 'd', 1: 'e', 2: 'h', 3: 'l', 4: 'o', 5: 'r', 6: 'w'}\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "# class to index mapping\n",
    "print(dataset_imgs.class_to_idx)\n",
    "\n",
    "# {'airplane': 0, 'car': 1, 'cat': 2, 'dog': 3, 'flower': 4, 'fruit': 5, 'motorbike': 6, 'person': 7}\n",
    "\n",
    "# index to class mapping : reverse of class to index mapping\n",
    "idx_to_class = {v: k for k, v in dataset_imgs.class_to_idx.items()}\n",
    "print(idx_to_class) # {0: 'airplane', 1: 'car', 2: 'cat', 3: 'dog', 4: 'flower', 5: 'fruit', 6: 'motorbike', 7: 'person'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13721dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes:  {'d': 100, 'e': 100, 'h': 100, 'l': 100, 'o': 100, 'r': 100, 'w': 100}\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    count_dict = {k:0 for k,v in dataset_imgs.class_to_idx.items()} # initialise dictionary\n",
    "    \n",
    "    for input, label in dataset:\n",
    "        label = idx_to_class[label]\n",
    "        count_dict[label] += 1\n",
    "            \n",
    "    return count_dict\n",
    "\n",
    "print(\"Distribution of classes: \", get_class_distribution(dataset_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62536b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "420\n",
      "140\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "train_percent = int(len(dataset_imgs)*0.6)\n",
    "val_percent = int(len(dataset_imgs)*0.2)\n",
    "test_percent = int(len(dataset_imgs)*0.2)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_imgs, (train_percent, val_percent, test_percent))\n",
    "print(len(dataset_imgs)) # length of the dataset\n",
    "print(len(train_dataset)) # length of the train division\n",
    "print(len(val_dataset)) # length of the validation division\n",
    "print(len(test_dataset)) # length of the test division\n",
    "\n",
    "# 700\n",
    "# 420\n",
    "# 140\n",
    "# 140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8cbd9",
   "metadata": {},
   "source": [
    "## Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6f3654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train_loader: 105\n",
      "Length of the val_loader: 35\n",
      "Length of the test_loader: 35\n"
     ]
    }
   ],
   "source": [
    "# Link: https://www.scaler.com/topics/pytorch/how-to-split-a-torch-dataset/\n",
    "\n",
    "# # first parameter: training_data\n",
    "# # shuffle means while training the data it will shuffle the data\n",
    "# # num_workers means parallelizing the process (4 different processes can work at the same time)\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=4, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=False, batch_size=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=4)\n",
    "\n",
    "# length of dataloader = overall dataset len / batch size\n",
    "print(\"Length of the train_loader:\", len(train_loader))\n",
    "print(\"Length of the val_loader:\", len(val_loader))\n",
    "print(\"Length of the test_loader:\", len(test_loader))\n",
    "\n",
    "# Length of the train_loader: 105\n",
    "# Length of the val_loader: 35\n",
    "# Length of the test_loader: 35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623e073",
   "metadata": {},
   "source": [
    "## Create the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f4d89f-c66d-473a-b526-d4cbb991339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 network trained on the ImageNet dataset\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer and then fine tune\n",
    "# this model on our custom data set to detect whether the given image to a\n",
    "# model is what letter\n",
    "# freezing all layers except the final classification layers which is \n",
    "# responsible for performing classfication\n",
    "# this for loop line specifically: return an iterator over module parameters,\n",
    "# yielding both the name of the parameter as well as the parameter itself\n",
    "# only the last layer is optimized, the rest will be frozen (base parameters\n",
    "# will receive no gradients - they have requires_grad=False)\n",
    "for name, param in model.named_parameters():\n",
    "    # if the paramater contains this FC (FC means fully connected layer), then set the required grads equal to \n",
    "    # true\n",
    "    if \"fc\" in name:  \n",
    "        # Unfreeze the final classification layer\n",
    "        # to achieve best results, we fine-tune the later layers in the network\n",
    "        # later, will replace the last layer\n",
    "        param.requires_grad = True\n",
    "    # if FC is not in the parameter, then set the required grads equal to false\n",
    "    else:\n",
    "        # freezes the layers and prevents PyTorch from calculating the gradients for these layers during\n",
    "        # back propogations\n",
    "        param.requires_grad = False\n",
    "    # what happens is wherever fc parameter is present, all those layers will be trained because we are setting\n",
    "    # the value true over there and wherever we have written false, all those layers will be freeze. this is how\n",
    "    # we freeze all the layers and we can only fine tune the final layer\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# whenever working on image classification, this is most commonly method to calculate loss. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer is a stochastic gradient descent optimizer (lr is learning range and momentum)\n",
    "# CAN CHANGE THESE VALUES to test how the model performs\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# sending all the models here to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3d5f0",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe788bcb-77d7-4c86-9f4b-80868208b8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], TRAIN Loss: 3.3304, Acc: 0.1190\n",
      "Epoch [1/10], TEST Loss: 2.0152, Acc: 0.2143\n",
      "Accuracy of the network on the 140 validation images: 21.428571701049805 %\n",
      "Epoch [2/10], TRAIN Loss: 1.8825, Acc: 0.3571\n",
      "Epoch [2/10], TEST Loss: 1.8119, Acc: 0.3500\n",
      "Accuracy of the network on the 140 validation images: 35.0 %\n",
      "Epoch [3/10], TRAIN Loss: 1.8536, Acc: 0.3405\n",
      "Epoch [3/10], TEST Loss: 1.7082, Acc: 0.3214\n",
      "Accuracy of the network on the 140 validation images: 32.14285659790039 %\n",
      "Epoch [4/10], TRAIN Loss: 1.6273, Acc: 0.4262\n",
      "Epoch [4/10], TEST Loss: 1.5346, Acc: 0.4143\n",
      "Accuracy of the network on the 140 validation images: 41.42856979370117 %\n",
      "Epoch [5/10], TRAIN Loss: 1.5168, Acc: 0.4357\n",
      "Epoch [5/10], TEST Loss: 1.5197, Acc: 0.4357\n",
      "Accuracy of the network on the 140 validation images: 43.57143020629883 %\n",
      "Epoch [6/10], TRAIN Loss: 1.4023, Acc: 0.5000\n",
      "Epoch [6/10], TEST Loss: 1.3023, Acc: 0.5357\n",
      "Accuracy of the network on the 140 validation images: 53.57143020629883 %\n",
      "Epoch [7/10], TRAIN Loss: 1.2761, Acc: 0.5429\n",
      "Epoch [7/10], TEST Loss: 1.2966, Acc: 0.5357\n",
      "Accuracy of the network on the 140 validation images: 53.57143020629883 %\n",
      "Epoch [8/10], TRAIN Loss: 1.3950, Acc: 0.4952\n",
      "Epoch [8/10], TEST Loss: 1.3193, Acc: 0.5000\n",
      "Accuracy of the network on the 140 validation images: 50.0 %\n",
      "Epoch [9/10], TRAIN Loss: 1.3607, Acc: 0.5238\n",
      "Epoch [9/10], TEST Loss: 1.4788, Acc: 0.4714\n",
      "Accuracy of the network on the 140 validation images: 47.14285659790039 %\n",
      "Epoch [10/10], TRAIN Loss: 1.2681, Acc: 0.5333\n",
      "Epoch [10/10], TEST Loss: 1.4152, Acc: 0.4714\n",
      "Accuracy of the network on the 140 validation images: 47.14285659790039 %\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# garbage collector\n",
    "import gc\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    ## TRAIN SET\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs means the image \n",
    "        # all being sent to device because the model is on the device (CPU/GPU)\n",
    "        inputs = inputs.to(device)\n",
    "        # labels means the output label (the class name)\n",
    "        labels = labels.to(device)\n",
    "        # clear the gradients from the previous iterations \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # if phase is train, then we are using the model to make predictions \n",
    "            # and providing the inputs which are the images \n",
    "            # prediction of the model is stored in outputs\n",
    "            # \"outputs\" is the predictions of the model and \"labels\" is the actual labels\n",
    "            outputs = model(inputs)\n",
    "            # so we are comparing the output label and our current label, on the basis\n",
    "            # of that we are getting the loss value\n",
    "            loss = criterion(outputs, labels)\n",
    "            # this line is responsible for showing you the predictions \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # if the phase is train, we perform backward pass\n",
    "            if train_loader:\n",
    "                # backward pass: calculating the gradients\n",
    "                loss.backward()\n",
    "                # and then updating the weights on the basis of calculated gradients \n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "        # storing all the losses and all the correct predictions \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "    # these two lines help you to see the epoch loss and epoch accuracy\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_correct.double() / len(train_dataset)\n",
    "    print ('Epoch [{}/{}], TRAIN Loss: {:.4f}, Acc: {:.4f}'.format(epoch+1, num_epochs, epoch_loss, epoch_acc))\n",
    "    \n",
    "    # delete and garbage collect\n",
    "    del inputs, labels, outputs\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    # TEST SET\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # reinitialize \n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            # inputs means the image \n",
    "            # all being sent to device because the model is on the device (CPU/GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            # labels means the output label (the class name)\n",
    "            labels = labels.to(device)\n",
    "            # clear the gradients from the previous iterations \n",
    "            optimizer.zero_grad()\n",
    "            # prediction of the model is stored in outputs\n",
    "            # \"outputs\" is the predictions of the model and \"labels\" is the actual labels\n",
    "            outputs = model(inputs)\n",
    "            # this line is responsible for showing you the predictions \n",
    "            _, preds = torch.max(outputs, 1) \n",
    "            # so we are comparing the output label and our current label, on the basis\n",
    "            # of that we are getting the loss value\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # storing all the losses and all the correct predictions \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "            running_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        # these two lines helping you to see the epoch loss and epoch accuracy\n",
    "        epoch_loss = running_loss / len(test_dataset)\n",
    "        epoch_acc = running_correct.double() / len(test_dataset)\n",
    "\n",
    "        print ('Epoch [{}/{}], TEST Loss: {:.4f}, Acc: {:.4f}'.format(epoch+1, num_epochs, epoch_loss, epoch_acc))\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(140, 100 * running_correct / total))\n",
    "        \n",
    "        # delete and garbage collect\n",
    "        del inputs, labels, outputs\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c696019-b8ca-4cac-9f5f-d350e7285265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'letter_classification_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057bb6d-9592-4809-af3a-fb05ef595477",
   "metadata": {},
   "source": [
    "## Perform Classification on Unseen Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f514-6ad8-46cb-b9b6-f56f7da59476",
   "metadata": {},
   "source": [
    "To classify unseen images, the model is loaded in and applied to unseen images from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a509530-539e-4e35-8a1b-181215da720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# the pretrained model is trained on imageNet dataset with a thousand classes \n",
    "# freezing all the layers except for the last layer means we are using transfer learning which\n",
    "# means the model is already trained on some data \n",
    "# that means this model is ready to extract the features so we can use that knowledge from the \n",
    "# pretrained model to extract the feature and in the final layer we are only using the two \n",
    "# neurons which are responsible for telling us what letter class it is\n",
    "\n",
    "# replacing the last layer to adapt the model to a new problem with a different number of output\n",
    "# classes\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('letter_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "# 7 for the number of classes\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 7)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557a6cc-c536-44b7-bfad-52f3d33e1480",
   "metadata": {},
   "source": [
    "Load in unseen image to prepare to perform inference using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the unseen image\n",
    "image_path = 'h_sign_lang.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "display(image)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "preprocessed_unseen = preprocess(image)\n",
    "# adding a batch dimension\n",
    "batch_unseen = preprocessed_unseen.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8ef12",
   "metadata": {},
   "source": [
    "Perform inference of the gesture \"h\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99206f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445dcf2-2311-4025-80e9-2b57ccf559e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the unseen image\n",
    "image_path = 'r_sign_lang.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "display(image)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "preprocessed_unseen = preprocess(image)\n",
    "# adding a batch dimension\n",
    "batch_unseen = preprocessed_unseen.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15505026-2f4d-4072-9ba1-c3dc265a43b1",
   "metadata": {},
   "source": [
    "Perform inference of the gesture \"r\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefe032-46f2-4f08-984e-39584a83cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0ed80",
   "metadata": {},
   "source": [
    "## Example of a Wrongly Predicted Letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the unseen image\n",
    "image_path = 'w_sign_lang.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "display(image)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "preprocessed_unseen = preprocess(image)\n",
    "# adding a batch dimension\n",
    "batch_unseen = preprocessed_unseen.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8992f57",
   "metadata": {},
   "source": [
    "Perform inference of the gesture \"w\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
