{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c93f8a",
   "metadata": {},
   "source": [
    "OVERALL From : https://www.youtube.com/watch?v=mn5QDKQ54dQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad6dc0b-c30f-4300-9e62-dad86e69148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5203138",
   "metadata": {},
   "source": [
    "following about data augmentation: https://pytorch.org/vision/main/transforms.html\n",
    "\n",
    "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982f34b8-1c35-4767-8c19-5ee446aab58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for data augmentation and normalization\n",
    "# https://pytorch.org/vision/0.9/transforms.html\n",
    "\n",
    "# Link: https://neptune.ai/blog/data-augmentation-in-python\n",
    "# Data augmentation is a technique that can be used to artificially\n",
    "# expand the size of a training set by creating modified data from the\n",
    "# existing one. It is a good practice to use DA if you want to prevent \n",
    "# overfitting, or the initial dataset is too small to train on, or even \n",
    "# if you want to squeeze better performance from your model.\n",
    "\n",
    "# Benefits of data augmenting:\n",
    "# 1. prevent overfitting\n",
    "# 2. improving the performance of the model \n",
    "\n",
    "# Techniques of data augmentation used here (for images):\n",
    "# 1. geometric transformations: randomly flip, crop, rotate images \n",
    "# 2. color space transformations: change RGB color channels\n",
    "\n",
    "transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # ADDED IN FROM https://pytorch.org/vision/0.9/transforms.html\n",
    "    \n",
    "        # horizontally flip the image randomly with given probability\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # randomly resize and crop all the input images to 224 pixel size\n",
    "        transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "        # rotate the image by angle\n",
    "        transforms.RandomRotation(degrees=90),\n",
    "        # randomly convert image to grayscale with probability of p\n",
    "        # (default 0.1)\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        # performs a random persepctive transformation of the image with\n",
    "        # given probability \n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "        # vertically flip the image randomly with given probability\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        # converting all the image data to a tensor BECAUSE PyTorch accepts\n",
    "        # the data in the form of tensor\n",
    "        # when image is transformed into PyTorch tensor, the pixel values \n",
    "        # are scaled between 0.0 and 1.0\n",
    "        # converts the PIL image with a pixel range of [0, 255] to a PyTorch\n",
    "        # FloatTensor of shape (C, H, W) with range [0.0, 1.0]\n",
    "        # this is a MANDATORY step\n",
    "        transforms.ToTensor(),\n",
    "        # Link : https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "        # normalizing images means transforming them into such values that\n",
    "        # the means and std dev of the image become 0.0 and 1.0 respectively\n",
    "        # normalization helps get data within a range and reduces the \n",
    "        # skewness which helps learn faster and better, and can also tackle\n",
    "        # the diminishing and exploding gradients problem\n",
    "        # first parameter is the mean, second is the std\n",
    "        # the mean and std of ImageNet are: mean = [0.485, 0.465, 0.406]\n",
    "        # and std = [0.229, 0.224, 0.225]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # repeat steps from training data to testing (validation) data \n",
    "    'test': transforms.Compose([    \n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "        transforms.RandomRotation(degrees=90),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# performing this augmentation allows us to provide a variety of different scenarios to a model so that our model will \n",
    "# learn to generalize in a better way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c86d400-80fe-41bf-bd98-c4d92f6036b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset ImageFolder\n",
      "    Number of datapoints: 700\n",
      "    Root location: dataset\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomGrayscale(p=0.1)\n",
      "               RandomPerspective(p=0.5)\n",
      "               RandomVerticalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           ), 'test': Dataset ImageFolder\n",
      "    Number of datapoints: 700\n",
      "    Root location: dataset\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               RandomRotation(degrees=[-90.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomGrayscale(p=0.1)\n",
      "               RandomPerspective(p=0.5)\n",
      "               RandomVerticalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )}\n",
      "560\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory\n",
    "# image_classification notebook is in the same source/repos directory in local and Jupyter Notebook directory\n",
    "# this Notebook has to be in the same location as the dataset folder\n",
    "dataset_directory = 'dataset'\n",
    "\n",
    "# # Create data loaders\n",
    "# # data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# # inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "# datasets_images = {img: datasets.ImageFolder(os.path.join(dataset_directory, img), transforms[img]) \n",
    "#                    for img in dataset_directory}\n",
    "\n",
    "# print(datasets_images)\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "# data loaders are responsible to load the data so we are loading the training and testing data \n",
    "# inside dataset folder, there are two folders called train and test containing all the folders of the images\n",
    "datasets_images = {img: datasets.ImageFolder(os.path.join(dataset_directory, img), \n",
    "                                          transforms[img]) for img in ['train', 'test']}\n",
    "print(datasets_images)\n",
    "\n",
    "# # 80% of the d's go into train\n",
    "train_size = int(0.8 * len(datasets_images['train']))\n",
    "test_size = len(datasets_images['train']) - train_size\n",
    "# datasets_images['train'] contains all original images in their respective folders\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(datasets_images['train'], [train_size, test_size])\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678a888b-f751-4e87-be61-55a92be361c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 700, 'test': 700}\n",
      "['d', 'e', 'h', 'l', 'o', 'r', 'w']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m class_names \u001b[38;5;241m=\u001b[39m datasets_images[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mclasses\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_names)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mclasses)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "# # first parameter: training_data\n",
    "# # shuffle means while training the data it will shuffle the data\n",
    "# # num_workers means parallelizing the process (4 different processes can work at the same time)\n",
    "# dataloaders = {img: DataLoader(datasets_images[img], batch_size=4, shuffle=True, num_workers=4) for img in ['train', 'test']}\n",
    "# datasets_sizes = {img: len(datasets_images[img]) for img in ['train', 'test']}\n",
    "# # number of images per \"train\" and \"test\" folder\n",
    "# print(datasets_sizes)\n",
    "\n",
    "# # number of folders and their names in the \"train\" and \"test\" folders\n",
    "# # these are the letters we are working with\n",
    "# class_names = datasets_images['train'].classes\n",
    "# print(class_names)\n",
    "\n",
    "# first parameter: training_data\n",
    "# shuffle means while training the data it will shuffle the data\n",
    "# num_workers means parallelizing the process (4 different processes can work at the same time)\n",
    "dataloaders = {img: DataLoader(datasets_images[img], batch_size=4, shuffle=True, num_workers=4) for img in ['train', 'test']}\n",
    "datasets_sizes = {img: len(datasets_images[img]) for img in ['train', 'test']}\n",
    "# number of images per \"train\" and \"test\" folder\n",
    "print(datasets_sizes)\n",
    "\n",
    "# number of folders and their names in the \"train\" and \"test\" folders\n",
    "# these are the letters we are working with\n",
    "class_names = datasets_images['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "print(train_dataset.classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f4d89f-c66d-473a-b526-d4cbb991339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\emily\\Anaconda2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 model\n",
    "# pretrained = true means we are using a pretrained model\n",
    "# picking the model from torch vision model zoo\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer and then fine tune this model on our custom data set \n",
    "# to detect whether the given image to a model is what letter\n",
    "# freezing all layers except the final classification layers which is responsible for performing classfication\n",
    "for name, param in model.named_parameters():\n",
    "    # if the paramater contains this FC (FC means fully connected layer), then set the required grads equal to \n",
    "    # true\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    # if FC is not in the parameter, then set the required grads equal to false\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    # what happens is wherever fc parameter is present, all those layers will be trained because we are setting\n",
    "    # the value true over there and wherever we have written false, all those layers will be freeze. this is how\n",
    "    # we freeze all the layers and we can only fine tune the final layer\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# whenever working on image classification, this is most commonly method to calculate loss. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer is a stochastic gradient descent optimizer (lr is learning range and momentum)\n",
    "# CAN CHANGE THESE VALUES to test how the model performs\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# sending all the models here to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe788bcb-77d7-4c86-9f4b-80868208b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.3654 Acc: 0.1524\n",
      "test Loss: 1.8379 Acc: 0.3476\n",
      "train Loss: 1.8918 Acc: 0.3405\n",
      "test Loss: 1.0831 Acc: 0.5833\n",
      "train Loss: 1.6896 Acc: 0.4167\n",
      "test Loss: 0.8927 Acc: 0.7238\n",
      "train Loss: 1.6497 Acc: 0.4429\n",
      "test Loss: 0.6354 Acc: 0.8238\n",
      "train Loss: 1.4886 Acc: 0.4595\n",
      "test Loss: 0.5863 Acc: 0.8048\n",
      "train Loss: 1.3913 Acc: 0.5167\n",
      "test Loss: 0.5167 Acc: 0.8405\n",
      "train Loss: 1.2768 Acc: 0.5476\n",
      "test Loss: 0.4021 Acc: 0.8762\n",
      "train Loss: 1.2814 Acc: 0.5357\n",
      "test Loss: 0.4010 Acc: 0.8762\n",
      "train Loss: 1.3050 Acc: 0.5238\n",
      "test Loss: 0.4085 Acc: 0.8571\n",
      "train Loss: 1.3541 Acc: 0.5119\n",
      "test Loss: 0.4406 Acc: 0.8381\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        # these two variables are used to store the loss and the correct \n",
    "        # predictions inside every epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            # inputs means the image \n",
    "            inputs = inputs.to(device)\n",
    "            # labels means the output label (the class name)\n",
    "            labels = labels.to(device)\n",
    "            # all being sent to device because the model is on the device.\n",
    "            # which device depends on whether this is running on CPU or GPU\n",
    "            \n",
    "            # clear the gradients from the previous iterations \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # if phase is train, then we are using the model to make predictions \n",
    "                # and providing the inputs which are the images \n",
    "                outputs = model(inputs)\n",
    "                # prediction of the model is stored in outputs\n",
    "                # this line is responsible for showing you the predictions \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                # now providing the outputs which are the predictions \n",
    "                # \"outputs\" is the predictions of the model and \"labels\" is the actual labels\n",
    "                # so we are comparing the output label and our current label, on the basis\n",
    "                # of that we are getting the loss value \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # how deep learning/CNN works: \n",
    "                # there is backward pass and forward pass. first we go the forward pass. \n",
    "                # forward pass means you provide input to the model, the model processes\n",
    "                # the image and provide you the corresponding label. then in back propogation, \n",
    "                # we have backward pass. during backward pass, we calculate the gradients and \n",
    "                # then update the weights and after that we again use the forward pass to work\n",
    "                # on the model again, train it with the updated weights. \n",
    "                \n",
    "                # if the phase is train, we will perform backward pass. \n",
    "                if phase == 'train':\n",
    "                    # in backward pass, we are calculating the gradients\n",
    "                    loss.backward()\n",
    "                    # and then we are updating the weights on the basis of calculated gradients \n",
    "                    optimizer.step()\n",
    "\n",
    "            # storing all the losses and all the correct predictions \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        # these two lines helping you to see the epoch loss and epoch accuracy\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c696019-b8ca-4cac-9f5f-d350e7285265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'letter_classification_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057bb6d-9592-4809-af3a-fb05ef595477",
   "metadata": {},
   "source": [
    "# Classification on Unseen Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f514-6ad8-46cb-b9b6-f56f7da59476",
   "metadata": {},
   "source": [
    "To use the saved model to classify unseen images, you need to load the model and then apply it to the new images for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a509530-539e-4e35-8a1b-181215da720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# the pretrained model is trained on imageNet dataset with a thousand classes \n",
    "# freezing all the layers except for the last layer means we are using transfer learning which\n",
    "# means the model is already trained on some data \n",
    "# that means this model is ready to extract the features so we can use that knowledge from the \n",
    "# pretrained model to extract the feature and in the final layer we are only using the two \n",
    "# neurons which are responsible for telling us what letter class it is\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('letter_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "# 7 for the number of classes\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 7)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557a6cc-c536-44b7-bfad-52f3d33e1480",
   "metadata": {},
   "source": [
    "Prepare your new image for classification. You should use the same data transformations you used during training. Here's an example of how to prepare an image for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1445dcf2-2311-4025-80e9-2b57ccf559e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT steps from training data also on this testing data \n",
    "\n",
    "# Load and preprocess the unseen image\n",
    "image_path = 'E_test_image.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# all these tasks are in preprocess variable\n",
    "# in preprocess, we want to preprocess the image\n",
    "input_tensor = preprocess(image)\n",
    "# adding a batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15505026-2f4d-4072-9ba1-c3dc265a43b1",
   "metadata": {},
   "source": [
    "Perform inference using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aefe032-46f2-4f08-984e-39584a83cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: e\n"
     ]
    }
   ],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # providing the input with input_batch\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['d', 'e', 'h', 'l', 'o', 'r', 'w']  # Make sure these class names match your training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657ea260-40c3-4a69-bb48-8de4fe305717",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display the image with the predicted class name\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Display the image with the predicted class name\n",
    "image = np.array(image)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.text(10, 10, f'Predicted: {predicted_class_name}', fontsize=12, color='white', backgroundcolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdda75-618a-4b16-884a-6b30d9a7d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
